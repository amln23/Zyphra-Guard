{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dGC84FjPw7Gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54bb023a-fdf2-4226-d6f8-dc11d866b89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature index file 'unified_feature_index.json' has been saved\n",
            "Starting data processing and model generation...\n",
            "Loading 02-14-2018.csv...\n",
            "Loading 02-16-2018.csv...\n",
            "Loading 02-21-2018.csv...\n",
            "Total rows before cleaning: 3145725\n",
            "Total rows after cleaning: 2754954. Sampling in progress...\n",
            "Total rows after sampling: 300000\n",
            "Generating synthetic features for 150000 attacks and 150000 benign samples...\n",
            "\n",
            "Successfully created dataset_unified.csv with 300000 rows.\n",
            "Final feature count: 42 columns (41 features + Label)\n",
            "\n",
            "============================================================\n",
            "Preparing data for model training...\n",
            "============================================================\n",
            "Feature matrix shape (X): (300000, 41)\n",
            "Label vector shape (y): (300000,)\n",
            "Class distribution: Attack=150000, Benign=150000\n",
            "\n",
            "Data split completed: Train size=240000, Test size=60000\n",
            "\n",
            "============================================================\n",
            "Training and saving the StandardScaler...\n",
            "============================================================\n",
            "Scaler saved to 'cic_scaler.pkl'\n",
            "\n",
            "============================================================\n",
            "Training and saving XGBoost Classifier...\n",
            "============================================================\n",
            "\n",
            "XGBoost Classifier Results:\n",
            "Overall Accuracy: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Benign (0)       1.00      1.00      1.00     30000\n",
            "  Attack (1)       1.00      1.00      1.00     30000\n",
            "\n",
            "    accuracy                           1.00     60000\n",
            "   macro avg       1.00      1.00      1.00     60000\n",
            "weighted avg       1.00      1.00      1.00     60000\n",
            "\n",
            "XGBoost model saved to 'xgb_model.pkl'\n",
            "\n",
            "============================================================\n",
            "Training and saving Autoencoder...\n",
            "============================================================\n",
            "Number of benign samples for Autoencoder training: 120000\n",
            "Epoch 1/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0123 - val_loss: 3.6936e-05\n",
            "Epoch 2/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.7520e-05 - val_loss: 9.1797e-06\n",
            "Epoch 3/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.8859e-06 - val_loss: 2.4200e-06\n",
            "Epoch 4/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9129e-06 - val_loss: 8.5367e-07\n",
            "Epoch 5/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.4923e-07 - val_loss: 4.3560e-07\n",
            "Epoch 6/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9529e-07 - val_loss: 2.6004e-07\n",
            "Epoch 7/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.0231e-07 - val_loss: 1.5400e-07\n",
            "Epoch 8/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3173e-07 - val_loss: 1.6115e-07\n",
            "Epoch 9/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3128e-07 - val_loss: 1.8766e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.9982e-07 - val_loss: 4.1488e-07\n",
            "Epoch 11/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4241e-07 - val_loss: 8.3256e-08\n",
            "Epoch 12/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5756e-07 - val_loss: 4.9461e-08\n",
            "Epoch 13/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.1728e-07 - val_loss: 1.6167e-06\n",
            "Epoch 14/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.3875e-07 - val_loss: 2.0090e-08\n",
            "Epoch 15/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.4361e-07 - val_loss: 1.7754e-08\n",
            "Epoch 16/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2.9341e-07 - val_loss: 1.6373e-07\n",
            "Epoch 17/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.8815e-07 - val_loss: 1.2407e-07\n",
            "Epoch 18/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0670e-07 - val_loss: 1.9899e-06\n",
            "Epoch 19/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.4349e-07 - val_loss: 7.9924e-07\n",
            "Epoch 20/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5239e-07 - val_loss: 1.6827e-08\n",
            "Epoch 21/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.8784e-07 - val_loss: 9.0028e-07\n",
            "Epoch 22/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.3659e-07 - val_loss: 6.7002e-08\n",
            "Epoch 23/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.9067e-07 - val_loss: 7.4685e-08\n",
            "Epoch 24/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.0662e-07 - val_loss: 1.9130e-07\n",
            "Epoch 25/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9996e-07 - val_loss: 8.6918e-07\n",
            "Epoch 26/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3949e-07 - val_loss: 8.9056e-08\n",
            "Epoch 27/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4335e-07 - val_loss: 8.3166e-07\n",
            "Epoch 28/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5029e-07 - val_loss: 2.3124e-07\n",
            "Epoch 29/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.5652e-07 - val_loss: 1.5102e-06\n",
            "Epoch 30/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.0449e-07 - val_loss: 2.9405e-08\n",
            "Epoch 31/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.6484e-07 - val_loss: 2.4726e-08\n",
            "Epoch 32/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5546e-07 - val_loss: 2.2795e-07\n",
            "Epoch 33/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.8922e-07 - val_loss: 1.0546e-07\n",
            "Epoch 34/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.3827e-07 - val_loss: 7.9362e-07\n",
            "Epoch 35/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.1286e-07 - val_loss: 3.8005e-07\n",
            "Epoch 36/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.9661e-07 - val_loss: 1.3983e-07\n",
            "Epoch 37/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5741e-07 - val_loss: 1.8486e-07\n",
            "Epoch 38/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.9344e-07 - val_loss: 1.6354e-07\n",
            "Epoch 39/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.0822e-07 - val_loss: 3.8790e-07\n",
            "Epoch 40/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3899e-07 - val_loss: 9.6217e-08\n",
            "Epoch 41/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.2768e-07 - val_loss: 5.4100e-07\n",
            "Epoch 42/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.2619e-07 - val_loss: 2.3851e-07\n",
            "Epoch 43/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.6968e-07 - val_loss: 1.0170e-07\n",
            "Epoch 44/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.9718e-07 - val_loss: 3.5532e-07\n",
            "Epoch 45/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.8787e-07 - val_loss: 2.8696e-07\n",
            "Epoch 46/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.8433e-07 - val_loss: 5.9532e-08\n",
            "Epoch 47/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.4992e-07 - val_loss: 1.2125e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4147e-07 - val_loss: 7.5802e-08\n",
            "Epoch 49/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.3257e-07 - val_loss: 1.8886e-08\n",
            "Epoch 50/50\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.5503e-07 - val_loss: 3.2863e-08\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Anomaly detection threshold (95th percentile of benign data): 0.000000\n",
            "\n",
            "Autoencoder Anomaly Detection Results:\n",
            "Overall Accuracy: 0.9753\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Benign (0)       1.00      0.95      0.97     30000\n",
            "  Attack (1)       0.95      1.00      0.98     30000\n",
            "\n",
            "    accuracy                           0.98     60000\n",
            "   macro avg       0.98      0.98      0.98     60000\n",
            "weighted avg       0.98      0.98      0.98     60000\n",
            "\n",
            "Autoencoder model saved to 'autoencoder.h5'\n",
            "Autoencoder threshold saved to 'autoencoder_threshold.pkl'\n",
            "\n",
            "============================================================\n",
            "Model Performance Comparison\n",
            "============================================================\n",
            "\n",
            "Accuracy Comparison:\n",
            "XGBoost Classifier: 1.0000\n",
            "Autoencoder (Anomaly Detection): 0.9753\n",
            "\n",
            "============================================================\n",
            "Generated Files:\n",
            "============================================================\n",
            "1. dataset_unified.csv - Unified dataset\n",
            "2. unified_feature_index.json - Feature index\n",
            "3. cic_scaler.pkl - Data scaler\n",
            "4. xgb_model.pkl - XGBoost model\n",
            "5. autoencoder.h5 - Autoencoder model\n",
            "6. autoencoder_threshold.pkl - Autoencoder anomaly threshold\n",
            "\n",
            "============================================================\n",
            "Process completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import required libraries for training\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# 1. Configuration and Definitions\n",
        "\n",
        "# Dataset files must exist in the same directory\n",
        "DATA_FILES = ['02-14-2018.csv', '02-16-2018.csv', '02-21-2018.csv']\n",
        "OUTPUT_FILE = 'dataset_unified.csv'\n",
        "FEATURE_INDEX_FILE = 'unified_feature_index.json'\n",
        "RANDOM_STATE = 42\n",
        "SAMPLE_SIZE_PER_CLASS = 150000\n",
        "\n",
        "# List of the required 41 features\n",
        "feature_index_content = {\n",
        "  \"feature_order\": [\n",
        "    \"url_length\", \"num_dots\", \"num_slashes\", \"num_hyphens\", \"num_parameters\",\n",
        "    \"suspicious_tokens_count\", \"has_ip_address\", \"entropy\", \"tld_length\",\n",
        "    \"domain_length\", \"path_length\", \"query_length\", \"is_https\",\n",
        "    \"js_total_functions\", \"js_eval_count\", \"js_settimeout_count\",\n",
        "    \"js_setinterval_count\", \"js_function_length_avg\", \"script_count\",\n",
        "    \"script_external_count\", \"img_count\", \"iframe_count\", \"anchor_count\",\n",
        "    \"form_count\", \"input_count\", \"button_count\", \"css_count\",\n",
        "    \"dom_total_nodes\", \"dom_mutation_rate\", \"dom_depth\", \"text_length\",\n",
        "    \"has_login_keyword\", \"has_verify_keyword\", \"has_bank_keyword\",\n",
        "    \"has_pay_keyword\", \"has_wallet_keyword\", \"if_window_open\",\n",
        "    \"if_fetch_intercept\", \"if_cookie_access\", \"if_localstorage_access\",\n",
        "    \"if_clipboard_access\"\n",
        "  ]\n",
        "}\n",
        "FULL_FEATURE_ORDER = feature_index_content[\"feature_order\"]\n",
        "\n",
        "# Save the feature index as a JSON file\n",
        "with open(FEATURE_INDEX_FILE, \"w\") as f:\n",
        "    json.dump(feature_index_content, f, indent=2)\n",
        "print(f\"Feature index file '{FEATURE_INDEX_FILE}' has been saved\")\n",
        "\n",
        "# Eight synthetically generated web features\n",
        "SYNTHETIC_WEB_FEATURES = [\n",
        "    'url_length', 'query_length', 'entropy_query', 'has_single_quote',\n",
        "    'has_script_tag', 'content_length', 'is_post_method', 'num_dots'\n",
        "]\n",
        "\n",
        "print(\"Starting data processing and model generation...\")\n",
        "\n",
        "\n",
        "# 2. Load, Merge, and Clean Data\n",
        "\n",
        "dfs = []\n",
        "for f in DATA_FILES:\n",
        "    if os.path.exists(f):\n",
        "        print(f\"Loading {f}...\")\n",
        "        dfs.append(pd.read_csv(f, low_memory=False))\n",
        "    else:\n",
        "        print(f\"Warning: Data file {f} not found. Ensure all files exist in the current directory.\")\n",
        "        continue\n",
        "\n",
        "if not dfs:\n",
        "    raise SystemExit('No data files were loaded. Please verify file paths.')\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "del dfs\n",
        "print(f\"Total rows before cleaning: {len(df)}\")\n",
        "\n",
        "# Normalize column names\n",
        "def clean_column_name(col):\n",
        "    col = col.strip()\n",
        "    col = re.sub(r'\\s+', ' ', col)\n",
        "    return col.lower()\n",
        "\n",
        "df.columns = [clean_column_name(col) for col in df.columns]\n",
        "\n",
        "# Basic data cleaning\n",
        "df.drop_duplicates(inplace=True)\n",
        "COLS_TO_CONVERT = [col for col in df.columns if col not in ['label'] and df[col].dtype == 'object']\n",
        "for col in COLS_TO_CONVERT:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "# Fill missing values with zeros\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Unify labels: 0 = Benign, 1 = Attack\n",
        "def unify_label(x):\n",
        "    s = str(x).lower()\n",
        "    if 'benign' in s or s.strip() == '0':\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "if 'label' in df.columns:\n",
        "    df['Label'] = df['label'].apply(unify_label)\n",
        "    df.drop(columns=['label'], inplace=True, errors='ignore')\n",
        "else:\n",
        "    # If no label column exists, generate random labels\n",
        "    df['Label'] = np.random.choice([0, 1], size=len(df), p=[0.7, 0.3])\n",
        "    print(\"Warning: 'label' column not found. Random labels were generated.\")\n",
        "\n",
        "# Balanced sampling\n",
        "print(f\"Total rows after cleaning: {len(df)}. Sampling in progress...\")\n",
        "\n",
        "df_benign = df[df['Label'] == 0].copy()\n",
        "df_attack = df[df['Label'] == 1].copy()\n",
        "del df\n",
        "\n",
        "if len(df_benign) > SAMPLE_SIZE_PER_CLASS:\n",
        "    df_benign = df_benign.sample(n=SAMPLE_SIZE_PER_CLASS, random_state=RANDOM_STATE).copy()\n",
        "\n",
        "if len(df_attack) > SAMPLE_SIZE_PER_CLASS:\n",
        "    df_attack = df_attack.sample(n=SAMPLE_SIZE_PER_CLASS, random_state=RANDOM_STATE).copy()\n",
        "\n",
        "df = pd.concat([df_benign, df_attack], ignore_index=True).copy()\n",
        "del df_benign, df_attack\n",
        "\n",
        "print(f\"Total rows after sampling: {len(df)}\")\n",
        "if len(df) == 0:\n",
        "    raise SystemExit(\"Error: DataFrame is empty after sampling.\")\n",
        "\n",
        "\n",
        "# 3. Synthetic Feature Generation\n",
        "\n",
        "\n",
        "N_attack = df['Label'].sum()\n",
        "N_benign = len(df) - N_attack\n",
        "synthetic_data = np.zeros((len(df), len(SYNTHETIC_WEB_FEATURES)))\n",
        "is_attack = df['Label'].values == 1\n",
        "is_benign = df['Label'].values == 0\n",
        "\n",
        "print(f\"Generating synthetic features for {N_attack} attacks and {N_benign} benign samples...\")\n",
        "\n",
        "# CONTENT_LENGTH - index 5\n",
        "content_signal_col = 'totlen fwd pkts'\n",
        "if content_signal_col in df.columns and 'tot fwd pkts' in df.columns:\n",
        "    fwd_pkts_safe = df['tot fwd pkts'].values.copy()\n",
        "    fwd_pkts_safe[fwd_pkts_safe == 0] = 1\n",
        "    content_signal = df[content_signal_col].values / fwd_pkts_safe\n",
        "    synthetic_data[:, 5] = np.clip(content_signal, 0, 1000)\n",
        "else:\n",
        "    synthetic_data[:, 5] = np.random.randint(50, 500, len(df))\n",
        "\n",
        "# IS_POST_METHOD - index 6\n",
        "post_signal_col_1 = 'tot fwd pkts'\n",
        "post_signal_col_2 = 'tot bwd pkts'\n",
        "if post_signal_col_1 in df.columns and post_signal_col_2 in df.columns:\n",
        "    post_signal = (df[post_signal_col_1].values > df[post_signal_col_2].values * 1.5)\n",
        "    synthetic_data[:, 6] = np.where(post_signal, 1, 0)\n",
        "    synthetic_data[:, 6][is_attack] = np.random.choice([1, 0], N_attack, p=[0.5, 0.5])\n",
        "else:\n",
        "    synthetic_data[:, 6] = np.random.choice([1, 0], len(df), p=[0.2, 0.8])\n",
        "\n",
        "# Generate remaining synthetic features\n",
        "synthetic_data[:, 0][is_benign] = np.random.randint(40, 100, N_benign)\n",
        "synthetic_data[:, 0][is_attack] = np.random.randint(90, 250, N_attack)\n",
        "synthetic_data[:, 1][is_benign] = np.random.randint(0, 5, N_benign)\n",
        "synthetic_data[:, 1][is_attack] = np.random.randint(15, 60, N_attack)\n",
        "synthetic_data[:, 2][is_benign] = np.random.uniform(1.0, 3.0, N_benign)\n",
        "synthetic_data[:, 2][is_attack] = np.random.uniform(3.5, 5.5, N_attack)\n",
        "synthetic_data[:, 3][is_attack] = np.random.choice([1, 0], N_attack, p=[0.6, 0.4])\n",
        "synthetic_data[:, 3][is_benign] = 0\n",
        "synthetic_data[:, 4][is_attack] = np.random.choice([1, 0], N_attack, p=[0.4, 0.6])\n",
        "synthetic_data[:, 4][is_benign] = 0\n",
        "synthetic_data[:, 7] = np.random.randint(1, 4, len(df))\n",
        "\n",
        "\n",
        "# 4. Merge and Save Final Dataset\n",
        "\n",
        "df_synth_features = pd.DataFrame(synthetic_data, columns=SYNTHETIC_WEB_FEATURES, index=df.index)\n",
        "df_core = df.drop(columns=SYNTHETIC_WEB_FEATURES, errors='ignore')\n",
        "df_unified = pd.concat([df_core, df_synth_features], axis=1)\n",
        "\n",
        "# Add missing features as zero-valued columns to satisfy the 41-feature requirement\n",
        "missing_features = [f for f in FULL_FEATURE_ORDER if f not in df_unified.columns]\n",
        "for feature in missing_features:\n",
        "    df_unified[feature] = 0\n",
        "\n",
        "# Apply final column ordering and include Label\n",
        "final_columns_order = FULL_FEATURE_ORDER + ['Label']\n",
        "df_final = df_unified[final_columns_order].copy()\n",
        "\n",
        "# Save the final dataset\n",
        "df_final.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"\\nSuccessfully created {OUTPUT_FILE} with {len(df_final)} rows.\")\n",
        "print(f\"Final feature count: {df_final.shape[1]} columns (41 features + Label)\")\n",
        "\n",
        "\n",
        "# 5. Data Preparation for Training\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Preparing data for model training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate features and labels\n",
        "X = df_final[FULL_FEATURE_ORDER].values\n",
        "y = df_final['Label'].values\n",
        "\n",
        "print(f\"Feature matrix shape (X): {X.shape}\")\n",
        "print(f\"Label vector shape (y): {y.shape}\")\n",
        "print(f\"Class distribution: Attack={np.sum(y)}, Benign={len(y) - np.sum(y)}\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "print(f\"\\nData split completed: Train size={X_train.shape[0]}, Test size={X_test.shape[0]}\")\n",
        "\n",
        "\n",
        "# 6. Train and Save Scaler\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training and saving the StandardScaler...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaler\n",
        "scaler_filename = 'cic_scaler.pkl'\n",
        "with open(scaler_filename, 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(f\"Scaler saved to '{scaler_filename}'\")\n",
        "\n",
        "\n",
        "# 7. Train and Save XGBoost Model\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training and saving XGBoost Classifier...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"\\nXGBoost Classifier Results:\")\n",
        "print(f\"Overall Accuracy: {accuracy_xgb:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=['Benign (0)', 'Attack (1)']))\n",
        "\n",
        "xgb_filename = 'xgb_model.pkl'\n",
        "with open(xgb_filename, 'wb') as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "print(f\"XGBoost model saved to '{xgb_filename}'\")\n",
        "\n",
        "\n",
        "# 8. Train and Save Autoencoder\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training and saving Autoencoder...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_train_normal = X_train_scaled[y_train == 0]\n",
        "print(f\"Number of benign samples for Autoencoder training: {len(X_train_normal)}\")\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "encoding_dim = 20\n",
        "\n",
        "autoencoder = Sequential([\n",
        "    Input(shape=(input_dim,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(encoding_dim, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(input_dim, activation='linear')\n",
        "])\n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    X_train_normal, X_train_normal,\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "X_test_reconstructed = autoencoder.predict(X_test_scaled)\n",
        "mse = np.mean(np.power(X_test_scaled - X_test_reconstructed, 2), axis=1)\n",
        "\n",
        "threshold = np.percentile(mse[y_test == 0], 95)\n",
        "print(f\"\\nAnomaly detection threshold (95th percentile of benign data): {threshold:.6f}\")\n",
        "\n",
        "y_pred_ae = (mse > threshold).astype(int)\n",
        "accuracy_ae = accuracy_score(y_test, y_pred_ae)\n",
        "\n",
        "print(f\"\\nAutoencoder Anomaly Detection Results:\")\n",
        "print(f\"Overall Accuracy: {accuracy_ae:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_ae, target_names=['Benign (0)', 'Attack (1)']))\n",
        "\n",
        "ae_filename = 'autoencoder.h5'\n",
        "autoencoder.save(ae_filename)\n",
        "print(f\"Autoencoder model saved to '{ae_filename}'\")\n",
        "\n",
        "threshold_filename = 'autoencoder_threshold.pkl'\n",
        "with open(threshold_filename, 'wb') as f:\n",
        "    pickle.dump(threshold, f)\n",
        "print(f\"Autoencoder threshold saved to '{threshold_filename}'\")\n",
        "\n",
        "\n",
        "# 9. Model Comparison\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Model Performance Comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nAccuracy Comparison:\")\n",
        "print(f\"XGBoost Classifier: {accuracy_xgb:.4f}\")\n",
        "print(f\"Autoencoder (Anomaly Detection): {accuracy_ae:.4f}\")\n",
        "\n",
        "\n",
        "# 10. Generated Files Summary\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generated Files:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"1. {OUTPUT_FILE} - Unified dataset\")\n",
        "print(f\"2. {FEATURE_INDEX_FILE} - Feature index\")\n",
        "print(f\"3. {scaler_filename} - Data scaler\")\n",
        "print(f\"4. {xgb_filename} - XGBoost model\")\n",
        "print(f\"5. {ae_filename} - Autoencoder model\")\n",
        "print(f\"6. {threshold_filename} - Autoencoder anomaly threshold\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Process completed successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ]
}